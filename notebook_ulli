Here i take my notes while working on the project to make it easier to set up the real text we want to submit.

Prep:
we made ourselfs familiar with the rl zoo and some of its models.
For running the real code we mainly use google colad for more claculating power and it is easier to reproduce for other people.


start:
Our first real task is to generate data from a different models in the same enviroment.
We use the different models of the zoo which were trained with different algorithms but on the same enviroment(PPO, a2c, ...).
to start we used the enviroment bipedalwalker to make it not too complex at the beginning.
But at the same time it is complex enough to find interesting behaiviors because the trained models ae not perfect like it is often the case with simpler enviroments.
The script enjoy.py uses the trained models automatically, you just have to specify the model and algorithm you want to use.
To the get the data we want we need to make some changes.
In this file we save the observations as well as the actions and reward after every step the model makes.
After running the models we want we safe the data in a csv file.
To analyse the data we use the python libary pandas.

Our next task after generating the data is to find patterns in the data.
Because it is not usefull to compare the whole datasets of the models we want to extract repetetive patterns in the data.
This could maybe be a subtask like walking one step or something which will be repeated all over he Episode.
We can then compare this subtask to the subtask an other agent used to solve the task.

For pattern recognition we tried to look at programms used in the stock market.
Many traders rely on patterns in the prize curve of their stocks to make predictions.
Unfortunatly those are always programed to find trading specific patterns and or also always based on a date/time scale.
Could be complicated to transfer our timesteps based data into that format.
Another main problem is that we got a multidimensional problem and all stock market solutions are always onedimensional.
Maybe we can easily overcome this buy rewriting the scripts to use arrays or running it multiple time.
We came accross the python libary prophet which will try to extract weekly, monthly and annualy patterns in ur data.
The problem is that the main task of our pattern recognition will also be to realize when a patern starts and not just plot possible patterns in pre arranged timeslots.
Other programms i came accross had different approches with thir own Problems.
For example you often have to specify an interval length in which the programm will look for patterns.
lets say 50 timesteps.
Then the program starts in the beginning and then saves the next 50 timesteps and then starts the same from timestep 2.
after that the programm will compare those "sup groups" and compare them to each other.
The problem here is that we dont know how long such a pattern is and the length can vary between the models.

Often those programms look for local maximums and minimums spikes and if they form specific patterns.
But i dont really see the use of that for our case.

A more mathematically approach would be if we use a fouriertransformation to find the frequencies of the Data functions and then just look at the lowest frequency.
Here we come accross some other problems.
First of all we got the problem that we work with diskrete datapoints and not a function which we would ne for a transformaton.
If we interpolate our data into a function we got the problem that we get a way to complicated function(if it exists) for all our data points.
if we smoothen the data out firstly or do not use all data points to not interpolate the complete noise. we come to new problems:
-is it just noise or are these mini spikes important to fullfill the task?


